{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import cv2 # OpenCV package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0825483, r0653687, r0823967, r0820202, r0822884</span>.\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "This notebook is structured as follows:\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. FaceNet\n",
    "6. Publishing best results\n",
    "7. Discussion\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "## 0.1. Loading data\n",
    "The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The training set contains 80 examples, the test set contains 1816 examples.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "train = pd.read_csv(\n",
    "    '../input/kul-h02a5a-computervision-groupassignment0/train_set.csv', index_col = 0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "test = pd.read_csv(\n",
    "    '../input/kul-h02a5a-computervision-groupassignment0/test_set.csv', index_col = 0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# Read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [cv2.cvtColor(np.load('../input/kul-h02a5a-computervision-groupassignment0/train/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in train.iterrows()]\n",
    "\n",
    "test['img'] = [cv2.cvtColor(np.load('../input/kul-h02a5a-computervision-groupassignment0/test/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in test.iterrows()]\n",
    "  \n",
    "\n",
    "train_size, test_size = len(train),len(test)\n",
    "\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n",
    "\n",
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set contains an identifier, name, image information and class label\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test set only contains an identifier and corresponding image information.\n",
    "\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class distribution in the training set:\n",
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n",
    "\n",
    "## 0.3. Preprocess data\n",
    "### 0.3.1 Example: HAAR face detector\n",
    "In this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's plot a few examples. First we will define a function to easy plot mulptiple images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to play with \n",
    "FACE_SIZE = (100, 100)\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7, cmap=\"brg\"):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i], cmap=cmap)\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i], cmap=cmap)\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i], cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will have to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed data using the HAAR features\n",
    "preprocessor = HAARPreprocessor(path = './', face_size=FACE_SIZE)\n",
    "\n",
    "train_X, train_y = preprocessor(train), train['class'].values\n",
    "test_X = preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.2 dlib face detector\n",
    "We can see that eventhough the HAAR preprocessing does detect faces, there are several images where it misses the face entirely. In this is the section we will suggest a different preprocessing to achieve better results. The [dlib library](http://dlib.net/), containes several functions which will allow us better extract faces from the images. Additionally we will make use of the face_utils function from the imutils library to easily convert the extracted faces to numpy arrays. First of all the libraries will need to be installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "import dlib\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a function to convert the bounding box from dlib into a format that OpenCV will understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # Convert dlib boudning box to the format (x, y, w, h) as we would normally do with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # Return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the get_frontal_face_detector from the dlib library. This is a ready to use face detection algorithm, based on the histogram of oriented gradients, which we will discuss in a later section. This function will return all extracted faces with their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def dlib(data,data_type = 'train'):\n",
    "    #We set the size of the image\n",
    "    dim = (160, 160)\n",
    "    data_images=[]\n",
    "    #If we are processing training data we need to keep track of the labels\n",
    "    if data_type=='train':\n",
    "        data_labels=[]\n",
    "    #Loop over all images\n",
    "    for cnt in range(0,len(data)):\n",
    "        image = data['img'][cnt]\n",
    "        #The large images are resized\n",
    "        if image.shape[0] > 1000 and image.shape[1] > 1000:\n",
    "            image = cv2.resize(image, (1000,1000), interpolation = cv2.INTER_AREA)\n",
    "        #The image is converted to grey-scales\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        #Take the smallest face\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            #Convert the bounding box to edges\n",
    "            (x, y, w, h) = rect_to_bb(rect)\n",
    "            #Here we copy and crop the face out of the image\n",
    "            clone = image.copy()\n",
    "            if(x>=0 and y>=0 and w>=0 and h>=0):\n",
    "                crop_img = clone[y:y+h, x:x+w]\n",
    "        #We resize the face to the correct size\n",
    "        resized = cv2.resize(crop_img, dim, interpolation = cv2.INTER_AREA)\n",
    "        data_images.append(resized)\n",
    "        #And add the label to the list\n",
    "        if data_type=='train':\n",
    "            data_labels.append(data['class'][cnt])\n",
    "    #Lastly we need to return the correct number of arrays\n",
    "    if data_type=='train':\n",
    "        return np.array(data_images), np.array(data_labels)\n",
    "    else:\n",
    "        return np.array(data_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract our features once more, but this time using dlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed data using dlib\n",
    "train_X , train_y = dlib(train, 'train')\n",
    "test_X = dlib(test,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's try again to plot a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in some images that the faces are not correct. This can happen if multiple faces are in an image, because the face detector cannot decide which of the faces is the correct one according to the label. When this happens we can manually edit the labels to prevent any noise in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainX = list()\n",
    "newTrainY = list()\n",
    "for index, face_pixels in enumerate(train_X[train_y == 0]):\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(0)\n",
    "    \n",
    "for index, face_pixels in enumerate(train_X[train_y == 1]):\n",
    "    if index not in [20,27,28]:\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(1)\n",
    "    \n",
    "for index, face_pixels in enumerate(train_X[train_y == 2]):\n",
    "    if index not in [9,14,15,18,22,23]:\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(2)\n",
    "    \n",
    "TrainX = np.array(newTrainX)\n",
    "TrainY = np.array(newTrainY)\n",
    "print(TrainX.shape, TrainY.shape)\n",
    "\n",
    "plot_image_sequence(TrainX, n=70, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Representations\n",
    "## 1.0. Example: Identify feature extractor\n",
    "Throughout the following sections we will define several classes for the feature extractors. Here we will show a simple example to demonstrate a generic structure. Our example feature extractor doesn't actually do anything... It just returns the input:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n",
    "\n",
    "It does make for a good placeholder and baseclass ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Baseline 1: HOG feature extractor\n",
    "In this section we will explore the Histogram Oriented Gradients (HOG) feature extractor. The idea of a feature extractor is to, as the name suggests, extract important features out of an image through the orientation of gradients. These features can then be compared to other images to recognise objects or in our case faces. To this end we will create a class which we shall name HOGFeatureExtractor. Upon initialisation of the function we will set several hyper parameters which we will discuss later. \n",
    "\n",
    "When the extractor is called each image will be converted to grey-scale. This allows the edges to be detected more easily. The edges are then converted to polar coordinates, which results in two matrices of the same size as the input image. The first matrix contains the magnitude of the gradient in each point and the second matrix contains the direction of this gradient. The next step is the histogram part of this extractor. Here all the gradients within a cell defined by the user, are gathered into a single histogram. Each bin of this histogram represents a predefined angle, here these angles are 0 to 180 in intervals of 20 degrees. All angles are set between 0 and 180 degrees rather than to 360 degrees. This means that both positive and negative gradients are added to the same angle. (According to source these so called unsigned gradients work better in object recognition that signed gradietns. ) The magnitudes of the gradients are divided amongst their corresponding bins according to their angle.\n",
    "\n",
    "Once the histogram is calculated for every cell, we have converted the image to sets of gradients. The strength of these gradients can depend on the lighting in the picture or shadows on the object. As we would like to determine the features of a face regardless of the light, we need to normalise the gradients. To normalise the gradients we will use block normalisation. This method determine the norm of a block of cells rather than the whole image. By only looking at a smaller region we can mitigate some of the effects of shadows. Once a block is normalised it is added as a one dimensional vector to the feature vector. These steps are then repeated for each block as it slides over the cells with a stride set by the user.\n",
    "\n",
    "Due to the order of the operations and the nature of the HOG features, they can be rather tricky to plot. In the class we have defined a plotting function which will take the correct features to overlay over an input image such that we can inspect which features are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOGFeatureExtractor(IdentityFeatureExtractor):\n",
    "    #Upon initialising the feature extractor will set several parameters which are needed lateron\n",
    "    def __init__(self, image_size = (100,100), cell_size = (3,3), block_size = (6,6), stride = (3,3)):\n",
    "        self.imgSize     = image_size\n",
    "        self.cellSize    = cell_size\n",
    "        self.blockSize   = (np.floor(block_size[0]/self.cellSize[0]),np.floor(block_size[1]/self.cellSize[1]))\n",
    "        self.blockStride = (np.floor(stride[0]/self.cellSize[0]),np.floor(stride[0]/self.cellSize[1]))\n",
    "        \n",
    "    #When the class is called it will turn images into HOG-feature vectors\n",
    "    def __call__(self, X):\n",
    "        features = []\n",
    "        #Here we check if a single image has passed or an array\n",
    "        if len(np.shape(X))<4:\n",
    "            X = [X]\n",
    "        #Loop over all images\n",
    "        for i in range(len(X)):\n",
    "            #First we convert the image to grey-scale\n",
    "            grey = cv2.cvtColor(np.float32(X[i]), cv2.COLOR_BGR2GRAY)\n",
    "            #Next we transfor the grey-scale image into a HOG-feature vector\n",
    "            features.append(self.transform(grey))\n",
    "        print(\"Extracted all HOG-feature vectors!\")\n",
    "        return features\n",
    "      \n",
    "    #The transform function will turn a single image into a vector of HOG-features\n",
    "    def transform(self, img):\n",
    "        #The edgdes are determined in the x and y direction\n",
    "        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
    "        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
    "        #The gradients are then converted to polar coordinates\n",
    "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "        #For each cell we will record a histogram\n",
    "        cells = np.zeros((int(np.floor(len(mag)/self.cellSize[0])),int(np.floor(len(mag[0])/self.cellSize[1])),9))\n",
    "        #Loop over all the cells\n",
    "        for i in range(len(cells[:,0,0])):\n",
    "            for j in range(len(cells[0,:,0])):\n",
    "                #Extracting the cell from the full image\n",
    "                cellMag = mag[i*self.cellSize[0]:(i+1)*self.cellSize[0],j*self.cellSize[1]:(j+1)*self.cellSize[1]]\n",
    "                cellAngle = angle[i*self.cellSize[0]:(i+1)*self.cellSize[0],j*self.cellSize[1]:(j+1)*self.cellSize[1]]\n",
    "                cells[i,j,:] = self.histogram(cellMag, cellAngle)\n",
    "        #The features need to be normalised per block\n",
    "        feat = [] \n",
    "        #Loop over all blocks\n",
    "        for i in range(int((len(cells[:,0])-self.blockSize[0])/self.blockStride[0]+1)):\n",
    "            for j in range(int((len(cells[0,:])-self.blockSize[1])/self.blockStride[1]+1)):\n",
    "                #Extract a block\n",
    "                block = (cells[int(i*self.blockStride[0]):int(i*self.blockStride[0]+self.blockSize[0]),\n",
    "                                       int(j*self.blockStride[1]):int(j*self.blockStride[1]+self.blockSize[1]),:])\n",
    "                #Turn the block into a 1D vector\n",
    "                block = np.ravel(block)\n",
    "                #In large areas where there are no gradients we just keep 0\n",
    "                if np.linalg.norm(block)==0:\n",
    "                    feat.append(np.zeros(np.shape(block)))\n",
    "                #Otherwise the block is normalised\n",
    "                else:\n",
    "                    feat.append(block/np.linalg.norm(block))\n",
    "        #All blocks are reshaped into a single 1D and returned\n",
    "        feat = np.ravel(feat)\n",
    "        return feat \n",
    "    \n",
    "    #The histogram function takes a cell and builds a histogram for the oriented gradients in this cell\n",
    "    def histogram(self, cellMag, cellAngle):\n",
    "        Hist = np.zeros(9)\n",
    "        #Loop over all elements in the cell\n",
    "        for i in range(len(cellMag[0,:])):\n",
    "            for j in range(len(cellMag[:,0])):\n",
    "                #First we find the lowest bin close to the angle\n",
    "                ind = int(np.floor(cellAngle[i,j]%180/20))\n",
    "                #The contribution to this bin is calculated and added\n",
    "                scale = 1 - (cellAngle[i,j]-ind*20)/20\n",
    "                Hist[ind] =+ scale*cellMag[i,j]\n",
    "                #The contribution to the following bin is calculated\n",
    "                scale = 1 - ((ind+1)*20-cellAngle[i,j])/20\n",
    "                #If the next bin would be 180 degrees, the contribution is added to 0 degrees\n",
    "                if ind == len(Hist)-1:\n",
    "                    Hist[0] =+ scale*cellMag[i,j]\n",
    "                else:\n",
    "                    Hist[ind] =+ scale*cellMag[i,j]\n",
    "        return Hist\n",
    "                     \n",
    "    #The plot function is used to show the HOG features of an image\n",
    "    def plot(self, feat, img = None):\n",
    "        #If a source image is give the HOG features will be plotted over the image\n",
    "        if img.any():\n",
    "            plt.imshow(cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2GRAY), cmap = 'gray')\n",
    "        #For ease of use the feature vector is reshaped to a vector of histograms\n",
    "        feat = feat.reshape(int(len(feat)/9),9)\n",
    "        #We will need the angles again as well\n",
    "        angles = np.float64([0, 20, 40, 60, 80,100,120,140,160])\n",
    "        #Looping over each cell in the image\n",
    "        for i in range(int(self.imgSize[0]/self.cellSize[0])):\n",
    "            for j in range(int(self.imgSize[1]/self.cellSize[1])):\n",
    "                #In the first block of rows the indices are counted different\n",
    "                if i < self.blockSize[1]:\n",
    "                    #Similarly in the first block of columns the indices are also counted different\n",
    "                    if j < self.blockSize[0]:\n",
    "                       currentHist = i*self.cellSize[0]+j\n",
    "                    else:\n",
    "                       currentHist = (self.blockSize[0]*self.blockSize[1]*(np.floor((j-self.blockSize[0])/self.blockStride[0])+1)\n",
    "                           +(j-self.blockSize[0])%self.blockStride[0]+(self.blockSize[0]-self.blockStride[0])+i*self.blockSize[0])\n",
    "                else:\n",
    "                    last = ((np.floor((self.imgSize[0]/self.cellSize[0]-self.blockSize[0])/self.blockStride[0])+1)*self.blockSize[0]*self.blockSize[1]\n",
    "                           *(np.floor((i-self.blockSize[1])/self.blockStride[1])+1))\n",
    "                    if j < self.blockSize[0]:\n",
    "                       currentHist = last+((i-self.blockSize[1])%self.blockStride[1]+(self.blockSize[1]-self.blockStride[1]))*self.blockSize[0]+j\n",
    "                    else:\n",
    "                       currentHist = (last+((i-self.blockSize[1])%self.blockStride[1]+(self.blockSize[1]-self.blockStride[1]))*self.blockSize[0]\n",
    "                                    +(self.blockSize[0]*self.blockSize[1]*(np.floor((j-self.blockSize[0])/self.blockStride[0])+1)\n",
    "                                    +(j-self.blockSize[0])%self.blockStride[0]+(self.blockSize[0]-self.blockStride[0])))\n",
    "                #We set the origin of each histogram at the centre of the corresponding cell\n",
    "                originX = []\n",
    "                originY = []\n",
    "                for l in range(9):\n",
    "                    originX.append((i+0.5)*self.cellSize[0])\n",
    "                    originY.append((j+0.5)*self.cellSize[1])\n",
    "                #Convert back to carthesian\n",
    "                x, y = cv2.polarToCart(feat[int(currentHist)], angles, angleInDegrees=True)\n",
    "                #Plot all positive\n",
    "                plt.quiver(originY,originX, y, x, scale_units='xy', scale=1.5/self.cellSize[0])\n",
    "                #Plot all negative\n",
    "                plt.quiver(originY,originX,-y,-x, scale_units='xy', scale=1.5/self.cellSize[0])\n",
    "        #If no source image was given the HOG features will be plotted upside down, so this fixes it\n",
    "        if not img.any():\n",
    "            plt.gca().invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have created our HOG-feature extractor, but now we still need to initialise an instance of our class. Here we will pass the input parameters. These parameters are all counted in number of pixels. This only needs to be done once, after which we can pass our images to the extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (160,160)\n",
    "cell_size  = (10,10)\n",
    "block_size = (20,20)\n",
    "stride = (10,10)\n",
    "\n",
    "hog  = HOGFeatureExtractor(image_size, cell_size, block_size, stride )\n",
    "feat = hog(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some examples. First we will increase the figure size, which will make the HOG-features clearer. To see a couple different images we will loop over 5 images. To plot the HOG features we will use the plotting function we included in the extractor class. Inorder to see how the features relate to the original image, we will pass the image as an arguement as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 50))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    hog.plot(feat[i],train_X[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature extractor finds the contours of the faces.The actual feature vector will contain much more information than we can display onto these images as the blocks in the normalising step overlap considerably. We can see immediately that if we were to roate the images by say 90 degrees the vectors will look completely different, making this method not rotation invariant. Similarly if the image would be of a different scale a single cell might describe a whole nose rather than a part of it, making this method also sensitive to scale. This problem however, is caught to some degree by our face detector which extracts all faces to the same size.\n",
    "\n",
    "### 1.1.1. Scale-Invariant Feature Transform \n",
    "Another method to describe faces is through Scale-Invariant Feature Transform (SIFT). We will not discuss the method in detail in order to not make this already lenghty notebook any longer, but for the sake of completeness, we will briefly discuss the outline of the method here. This method will search for keypoints in the training images and compare them to keypoints in new images. Opencv provides this method, but to easily apply it to all of our images we will define a new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT(X ):\n",
    "    keypoints = []\n",
    "    #Here we check if a single image has passed or an array\n",
    "    if len(np.shape(X))<4:\n",
    "        X = [X]\n",
    "    #Loop over all images\n",
    "    for i in range(len(X)):\n",
    "        #The images are converted to grey-scale and the correct format\n",
    "        grey= cv2.cvtColor(np.float32(X[i]), cv2.COLOR_BGR2GRAY).astype('uint8')\n",
    "        #Here we extract the keypoints\n",
    "        kp, des = cv2.SIFT_create().detectAndCompute(grey,None)\n",
    "        keypoints.append(kp)\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the extractor defined, let us extract the keypoints and look at a few examples again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp  = SIFT(train_X)\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    #Here we also need to convert the image to a format opencv understands\n",
    "    img = cv2.cvtColor(np.float32(train_X[i]), cv2.COLOR_BGR2GRAY).astype('uint8')\n",
    "    img = cv2.drawKeypoints(img,kp[i],img)\n",
    "    plt.imshow(img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These keypoints can then be combined into a dictionary of descriptions following a Bag of Words model. With this dictionary each face can be described by a set of \"words\". These words can then be passed to a classifier to recognise the different faces in these images. As mentioned before, we will not discuss this method in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. t-SNE Plots\n",
    "The feature vectors have a very high dimension (usually in the thousands) and this is rather difficult (actually impossible) to visualise. For this reason we will try to reduce the number of dimensions. We can do this through t-distributed stochastic neighbor embedding (t-SNE). In short, this method will construct probabilities such that similar vectors will have high probabilities. This method however, does not work well when we reduce the number of dimensions so drastically. This is why it is suggested [(scikit documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to first reduce the dimensions using principal component analysis. Though even when applying this method it is near impossible to represent this high number of dimensions in any meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Finding the 50 most important components from the  \"+str(len(feat[0]))+\" dimensions.\")\n",
    "pca = PCA(n_components=50)\n",
    "pca_res = pca.fit_transform(feat)\n",
    "print(\"Compressing  50 principle components into 2 dimensions.\")\n",
    "feat_embedded = TSNE(n_components=2,init='pca').fit_transform(pca_res)\n",
    "plt.scatter(feat_embedded[train_y == 0][:,0],feat_embedded[train_y == 0][:,1], color='r', label=\"0\")\n",
    "plt.scatter(feat_embedded[train_y == 1][:,0],feat_embedded[train_y == 1][:,1], color='g', label=\"1\")\n",
    "plt.scatter(feat_embedded[train_y == 2][:,0],feat_embedded[train_y == 2][:,1], color='b', label=\"2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor\n",
    "An important and interesting technique for face image representation is based upon principal component analysis, or PCA. It is a dimensionality reduction method that given a space of D dimensions finds k principal components, or vectors (with k << D) that can be used to remap the input data into a new smaller space still accounting for a good amount of data variance, or information. In this context however, such vectors can be considered \"faces\", in literature called EigenFaces. A face can be described as linear combination of these eigenfaces and the associated coefficients determine the vectorization of the face itself.\n",
    "It is possible to argue that SVD, another dimensionality reduction method, is better performing than PCA because it doesn't require matrix multiplication. However, by centering our input data to 0 (subtracting the average face) PCA behaves as SVD. Indeed, sklearn PCA implementation uses SVD to return the principal components.\n",
    "In this section we will show how to create such space, extract these features for each image and also how we ca reconstruct the input image as linear combination of the chosen eigenfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, size, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # initialize PCA with the specified number of components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        \n",
    "        self.mean_face = None\n",
    "        self.isTrain = True\n",
    "        self.size = size\n",
    "        \n",
    "    def __preprocess(self, X):\n",
    "        # to grayscale\n",
    "        X = np.array(np.mean(X, -1))\n",
    "        \n",
    "        # reshape to img_count * width x height\n",
    "        d = X.shape[1] * X.shape[2]\n",
    "        X = np.reshape(X, (X.shape[0], d))\n",
    "        \n",
    "        # make sure that each value is within [0, 255]\n",
    "        X = np.clip(X, 0, 255)\n",
    "        \n",
    "        # if the mean face has not been computed, do so as the \n",
    "        # numerical mean of each input image\n",
    "        if self.mean_face is None:\n",
    "            self.mean_face = np.mean(X, axis=0)\n",
    "            \n",
    "        # subtract the mean face to each input face\n",
    "        X -= self.mean_face\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    # plot mean face as image\n",
    "    def plot_mean_face(self):\n",
    "        plt.title(\"Mean face\")\n",
    "        plt.imshow(np.reshape(self.mean_face, self.size), cmap=\"gray\")\n",
    "        plt.show()\n",
    "    \n",
    "    # plot all the principal components as images --> eigenfaces\n",
    "    def plot_eigenfaces(self, imgs_per_row):\n",
    "        data = []\n",
    "        for comp in self.pca.components_:\n",
    "            data.append(np.reshape(comp, self.size))\n",
    "        \n",
    "        plot_image_sequence(data, len(data), imgs_per_row=7, cmap=\"gray\")\n",
    "        \n",
    "    # plot the cumulative explained variace ratio and return the first\n",
    "    # number of components that ensure at least min_var_exp of variance \n",
    "    # explained\n",
    "    def plot_explained_var(self, min_var_exp=0.9):\n",
    "        cumsum = self.pca.explained_variance_ratio_.cumsum()\n",
    "        plt.title(\"Cumulative explained variance over components\",size=15)\n",
    "        plt.xlabel('Number of components', size=12)\n",
    "        plt.ylabel('Cumulative explained variance', size=12)\n",
    "        plt.plot(cumsum)\n",
    "        plt.show()\n",
    "        \n",
    "        for i, c in enumerate(cumsum, 1):\n",
    "            if c >= min_var_exp:\n",
    "                return i\n",
    "    \n",
    "    # reconstruct image i by using eigenfaces from 1 to n_components\n",
    "    def image_reconstruction(self, i, x_trans):\n",
    "        comps = self.pca.components_\n",
    "        features = x_trans[i]\n",
    "\n",
    "        rec_steps = []\n",
    "        for n_comps in range(len(self.pca.components_)):\n",
    "            rec = np.dot(features[:n_comps+1], comps[:n_comps+1])\n",
    "            rec_steps.append(np.reshape(rec, self.size))\n",
    "\n",
    "        plot_image_sequence(rec_steps, len(rec_steps), cmap=\"gray\")\n",
    "        \n",
    "    # transform the input images in n_components features by using PCA\n",
    "    def transform(self, X):\n",
    "        X = self.__preprocess(X)\n",
    "        \n",
    "        # if we are in train mode we fit the PCA model on the images\n",
    "        if self.isTrain:\n",
    "            isTrain = False\n",
    "            self.pca.fit(X)\n",
    "            \n",
    "        # otherwise we transform only\n",
    "        return self.pca.transform(X)\n",
    "        \n",
    "    def inverse_transform(self, X):\n",
    "        # from the eigen vectors reconstruct the image\n",
    "        return self.pca.inverse_transform(X)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before apllying PCA to the images, we apply so,e preprocessing. In particular we do the following:\n",
    "* grayscale conversion: each pixel is associated with one value only and not 3 (RGB channels)\n",
    "* reshaping: each image is a one dimensional vector and not a matrix. This is required for feeding skitlearn the right type of data\n",
    "* value clipping: each value of such vector is between 0 and 255. It is done just to be sure that we are dealing with correct data.\n",
    "* data normalization: we first compute the mean face as vectorwise mean and then we subtract it from each input image in order to center them to 0. This final step ensures mathematical constrains for using PCA correctly and in the more efficient way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine a good number of eigenfaces to use, we can plot the cumulative explained variance over the number of original components and pick a component count that accounts for the wanted percentage of variance, in this case 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ = PCAFeatureExtractor(image_size)\n",
    "x_trans = pca_.transform(train_X)\n",
    "best_comp_count = pca_.plot_explained_var(min_var_exp=0.9)\n",
    "print(\"Best number of components:\", best_comp_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also find an appropriate number of components by looking at the average reconstruction loss. This loss defines the distance between the image reconstructed by using the specified number of eigenfaces and the original image itself. An ideal value is close to 0 as it means close to perfect reconstruction from the eigenfaces. In reality this doesn't happen, especially if the number of eigenfaces is far from the original number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best number of components based on reconstruction loss\n",
    "def comps_rec_loss():\n",
    "    min_loss = 1000\n",
    "    best_j = -1\n",
    "    losses = []\n",
    "    for j in range(80):\n",
    "        pca_ = PCAFeatureExtractor(j + 1)\n",
    "        x_trans = pca_.transform(train_X)\n",
    "        reconv = pca_.inverse_transform(x_trans)\n",
    "        vals = []\n",
    "        for i in range(train_X.shape[0]):\n",
    "            if np.mean(np.reshape(np.mean(train_X[i], -1), -1) - reconv[i]) >= 0:\n",
    "                vals.append(np.mean(np.reshape(np.mean(train_X[i], -1), -1) - reconv[i]))\n",
    "            else:\n",
    "                continue\n",
    "        loss = np.mean(vals) \n",
    "        losses.append(loss)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            best_j = j + 1\n",
    "\n",
    "    print(\"Best number of components:\", best_j)\n",
    "    print(\"Reconstruction loss: %.3f\" % min_loss)\n",
    "    plt.plot(np.arange(80), losses)\n",
    "    plt.show()\n",
    "\n",
    "comps_rec_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such method suggests only a few principal components. However, since the difference of reconstruction loss is very small across all components (the difference is in 1e-6 range), we can stick to the previously found value as it explains a good amount of variance without causing any difference in terms of reconstruction loss. In addition, the found value still reduces the problem dimensions making the computations faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Eigenface Plots\n",
    "Now that we have the right amount of features, capable to explain more than 90% of the variance of our data, we can plot the associated eigen vectors. As sasid, these vectors can be shown as face images, also known as eigenfaces. The first image is the average face extracted from the train set (and actually subtracted from each train set image). The other images are the sorted eigenfaces that the model has produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ = PCAFeatureExtractor(image_size, best_comp_count)\n",
    "x_trans = pca_.transform(train_X)\n",
    "pca_.plot_mean_face()\n",
    "pca_.plot_eigenfaces(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found eigenfaces can be used to reconstruct the original image by multiplying them with the PCA representation of the image itself. It is interesting to see how the reconstruction done in this way improves the more eigenfaces we use. Below we can see the original image and the reconstructed ones that use more and more eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image reconstruction\n",
    "image_num = 2\n",
    "plt.imshow(np.mean(train_X[image_num], -1), cmap=\"gray\")\n",
    "plt.show()\n",
    "pca_.image_reconstruction(image_num, x_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Feature Space Plots\n",
    "After all, images are transformed into vectors, so by using the first two dimensions of these vectors (associated to the first two eigenfaces) we can plot them into a 2D graph in order to better see if the model is able to discriminate between different classes, keeping close related images nonetheless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def imscatter(x, y, image, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    im = OffsetImage(image, zoom=zoom)\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0 in zip(x, y):\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists\n",
    "\n",
    "# take the first 2 features for each transformed image\n",
    "coords = x_trans[:, :2]\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# plot the original image based on the position defined by the first\n",
    "# two features of the transformed images\n",
    "for coords, img in zip(coords, train_X):\n",
    "    imscatter(coords[0], coords[1], img, ax=ax, zoom = 0.6)\n",
    "    ax.scatter(coords[0], coords[1])\n",
    "    \n",
    "plt.title('Projecting original images in 2D eigenspace', fontsize=40)       \n",
    "plt.xlabel(\"1st eigenface\", fontsize=20)\n",
    "plt.ylabel(\"2nd eigenface\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such 2D visualization unfortunately doesn't show good discriminative properties since images of different classes are blended together. However, it shows some robustness as female faces are generally put towards the top of the representation while male faces towards the bottom. In addition, non-face images (befere we removed them from the input data) were clearly separated from the others. We can still see this by taking a look at the right most images, which, due to facial expressions or items like glasses, are recognized as the model to be slightly different from the rest of the data. We have to keep in mind that this representation considers only the first two dimensions of our vectorized images, so we cannot ensure that what we see here defines exactly what happens under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Metrics\n",
    "As an evaluation metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but can give a wrong impression when dealing with unbalanced classes. As we already know that each class has roughly the same amount of examples in our traning set this will not be a problem here, though it is important to keep these limitations in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifiers\n",
    "## 3.0. Example: The *'not so smart'* classifier\n",
    "This random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set. Clearly this classifier will not be useful in any real face detection, but a simple example can be useful to easily see the structure of a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"Random classifier, draws a random sample based on class distribution observed \n",
    "    during training.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            Training set\n",
    "        y : array\n",
    "            Training set labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : RandomClassificationModel\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n",
    "        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Samples labels for the input data. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            dataset\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_star : array\n",
    "            'Predicted' labels\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        X = np.array(X)\n",
    "        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Random Forest Classifier\n",
    "Now that we have our feature vectors for each image we can go ahead and train a classification model that can make good use of such representation. This classifier is actually a combination of two inner classifiers, both random forests. Each forest has to learn how to correctly classify class 1 or class 2. Anything else that does not belong to their assigned class should be returned as 0. Then for each classification we compare the results in the following way:\n",
    "* if forest1 says that it's 1 and forest2 says that it is something that doesn't know we assign 1\n",
    "* if the opposite happens (forest1 = 0 and forest2 = 2) we assign 2\n",
    "* in any other case, either they both agree on 0 or they disagree, we return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "class RandomForest:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # init classifiers\n",
    "        self.clf1 = RandomForestClassifier(max_depth=3, random_state=0) # binary classifier. Correct class 1 else 0\n",
    "        self.clf2 = RandomForestClassifier(max_depth=3, random_state=0) # binary classifier. Correct class 2 else 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_class1 = [v if v == 1 else 0 for v in y] # labels of the first forest are set to either 1 or 0\n",
    "        y_class2 = [v if v == 2 else 0 for v in y] # here are instead set to 2 or 0\n",
    "        \n",
    "        self.clf1.fit(X, y_class1)\n",
    "        self.clf2.fit(X, y_class2)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred1 = self.clf1.predict(X)\n",
    "        pred2 = self.clf2.predict(X)\n",
    "        \n",
    "        preds = []\n",
    "        for p1, p2 in zip(pred1, pred2):\n",
    "            if p1 == 0 and p2 == 2:\n",
    "                preds.append(2)\n",
    "            elif p1 == 1 and p2 == 0:\n",
    "                preds.append(1)\n",
    "            else:\n",
    "                preds.append(0)\n",
    "        return preds\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiments\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n",
    "</div>\n",
    "<br>\n",
    "In this section we will have a look at how well each of these feature extractors performs. As we are not given the labels for the test set, we cannot test the accuracy on unseen data. Instead we will pass the training data through the classifier. This will end up giving us a very optimistic view of the used methods as it already knows these exact examples, but nevertheless it can give us a rough idea of the different methods.\n",
    "\n",
    "## 4.1. HOG Features\n",
    "First let us test the accuracy of the HOG-feature extractor with the RandomClassificationModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extractor and the classifier\n",
    "feature_extractor =  HOGFeatureExtractor(image_size=np.shape(train_X[0]), cell_size=(10,10), block_size=(20,20), stride=(10,10))\n",
    "classifier = RandomClassificationModel()\n",
    "\n",
    "# Train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# Evaluate performance of the model on the training set\n",
    "train_y_star = classifier.predict(feature_extractor(train_X)) # evaluation on train set\n",
    "\n",
    "print(\"Accuracy on train set: %.3f\" % accuracy_score(train_y, train_y_star))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurpisingly this classifier performs very poorly in most cases seeing as we simply guess the class at random. So let us try again with the RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extractor and the classifier\n",
    "feature_extractor =  HOGFeatureExtractor(image_size=np.shape(train_X[0]), cell_size=(10,10), block_size=(20,20), stride=(10,10))\n",
    "classifier = RandomForest()\n",
    "\n",
    "# Train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# Model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))\n",
    "\n",
    "# Evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "print(\"Accuracy on train set: %.3f\" % accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perfect accuracy on the otherhand does not mean it will always predict the correct face, rather it can perfectly recognise the faces in this training set. This almost always points towards the fact that it overfits the training data. Unfortunately we cannot derive any information about how well this generalises without a test set.\n",
    "\n",
    "To creat the predictions on the test set we can simply pass it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We are commenting the execution since this prediction is not used in final submission for saving time. \n",
    "    Please feel free to uncomment and execute the code\n",
    "'''\n",
    "\n",
    "# predict the labels for the test set \n",
    "#test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. PCA Features\n",
    "Next we look at the features extraced from the PCA method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extractor and the classifier\n",
    "feature_extractor = PCAFeatureExtractor(image_size, best_comp_count) \n",
    "classifier = RandomForest()\n",
    "\n",
    "# Train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# Model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))\n",
    "\n",
    "# Evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we can predict the labels for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We are commenting the execution since this prediction is not used in final submission for saving time. \n",
    "    Please feel free to uncomment and execute the code\n",
    "'''\n",
    "\n",
    "# predict the labels for the test set \n",
    "#test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FaceNet\n",
    "To really push the limits of face detection we will look at some state of the art methods. Modern day face extraction techniques have made use of Deep Convolution Networks. As we all know that features created by modern deep learning frameworks are really better than most handcrafted features. We checked 4 deep learning models namely, FaceNet (Google), DeepFace (Facebook), VGGFace (Oxford) and OpenFace (CMU). Out of these 4 models  [FaceNet](https://arxiv.org/pdf/1503.03832.pdf)  was giving us the best result. In general, FaceNet gives better result than all the other 3 models.\n",
    "\n",
    "<div style=\"width:100%; height:500px\">\n",
    "    <img src=\"https://d3i71xaburhd42.cloudfront.net/b2b0a001cf247691b3b130efa31f50ceb3ff758f/3-TableII-1.png\" width = 600px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "FaceNet uses the following architecture:\n",
    "\n",
    "<div style=\"width:100%; height:500px\">\n",
    "    <img src=\"https://developer.ridgerun.com/wiki/images/thumb/e/eb/Googlenet.png/1800px-Googlenet.png\" width = 1200px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "FaceNet uses inception module in blocks to reduce the number of trainable parameters. This model takes RGB images of 160x160 and generates an embedding of size 128 for an image. For this implementation we will need a couple extra functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cmake in c:\\program files\\python37\\lib\\site-packages (3.18.4.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dlib in c:\\program files\\python37\\lib\\site-packages (19.21.1)\n"
     ]
    }
   ],
   "source": [
    "## from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "!pip install cmake\n",
    "!pip install dlib\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Improving the preprocessing\n",
    "DLIB is a widely used model to detecting faces. In our experiments we found that dlib produces better results than HAAR, though we noticed some improvements could still be made:\n",
    "* If rectangle face bounds move out of image, we take the whole images instead of the face cropping. It is implemented as follows:\n",
    "  * if (x>=0 and y>=0 and w>=0 and h>=0):\n",
    "     * crop_img = clone[y:y+h, x:x+w]\n",
    "  * else:\n",
    "     * crop_img = clone.copy()\n",
    "* For test images, instead of saving one face per image we are saving all the faces for prediction.\n",
    "* Rather than a HOG based detector, we can use a CNN based detector.\n",
    "As these improvements are tailored to optimise for use with FaceNet, we will define a new corrected face detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.cnn_face_detection_model_v1(\"../input/pretrained-models-faces/mmod_human_face_detector.dat\")\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.rect.left()\n",
    "    y = rect.rect.top()\n",
    "    w = rect.rect.right() - x\n",
    "    h = rect.rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "def dlib_corrected(data, data_type = 'train'):\n",
    "    #We set the size of the image\n",
    "    dim = (160, 160)\n",
    "    data_images=[]\n",
    "    #If we are processing training data we need to keep track of the labels\n",
    "    if data_type=='train':\n",
    "        data_labels=[]\n",
    "    #Loop over all images\n",
    "    for cnt in range(0,len(data)):\n",
    "        image = data['img'][cnt]\n",
    "        #The large images are resized\n",
    "        if image.shape[0] > 1000 and image.shape[1] > 1000:\n",
    "            image = cv2.resize(image, (1000,1000), interpolation = cv2.INTER_AREA)\n",
    "        #The image is converted to grey-scales\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #Detect the faces\n",
    "        rects = detector(gray, 1)\n",
    "        sub_images_data = []\n",
    "        #Loop over all faces in the image\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            #Convert the bounding box to edges\n",
    "            (x, y, w, h) = rect_to_bb(rect)\n",
    "            #Here we copy and crop the face out of the image\n",
    "            clone = image.copy()\n",
    "            if(x>=0 and y>=0 and w>=0 and h>=0):\n",
    "                crop_img = clone[y:y+h, x:x+w]\n",
    "            else:\n",
    "                crop_img = clone.copy()\n",
    "            #We resize the face to the correct size\n",
    "            rgbImg = cv2.resize(crop_img, dim, interpolation = cv2.INTER_AREA)\n",
    "            #In the test set we keep track of all faces in an image\n",
    "            if data_type == 'train':\n",
    "                sub_images_data = rgbImg.copy()\n",
    "            else:\n",
    "                sub_images_data.append(rgbImg)\n",
    "        #If no face is detected in the image we will add a NaN\n",
    "        if(len(rects)==0):\n",
    "            if data_type == 'train':\n",
    "                sub_images_data = np.empty(dim + (3,))\n",
    "                sub_images_data[:] = np.nan\n",
    "            if data_type=='test':\n",
    "                nan_images_data = np.empty(dim + (3,))\n",
    "                nan_images_data[:] = np.nan\n",
    "                sub_images_data.append(nan_images_data)\n",
    "        #Here we add the the image(s) to the list we will return\n",
    "        data_images.append(sub_images_data)\n",
    "        #And add the label to the list\n",
    "        if data_type=='train':\n",
    "            data_labels.append(data['class'][cnt])\n",
    "    #Lastly we need to return the correct number of arrays\n",
    "    if data_type=='train':\n",
    "        return np.array(data_images), np.array(data_labels)\n",
    "    else:\n",
    "        return np.array(data_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will need to extract the features.\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> This block takes lot of time to execute due to the use of  cnn_face_detection_model_v1 in dlib which is memory intensive.  \n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_test_X  = dlib_corrected(test,'test')\n",
    "train_X , train_y = dlib_corrected(train, 'train')\n",
    "train_X  = train_X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_X.npy', train_X)\n",
    "np.save('train_y.npy', train_y)\n",
    "np.save('test_X.npy', corrected_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that few faces were still not good to be used as training data. These \"bad\" faces were again manually found and removed from the training data. This reduces the number of positive training examples we have, but will also reduce the noise in our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainX = list()\n",
    "newTrainY = list()\n",
    "for index, face_pixels in enumerate(train_X[train_y == 0]):\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(0)\n",
    "    \n",
    "for index, face_pixels in enumerate(train_X[train_y == 1]):\n",
    "    if index not in [8,14,18,19,21,24]:\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(1)\n",
    "    \n",
    "for index, face_pixels in enumerate(train_X[train_y == 2]):\n",
    "    if index not in [0,7,22,23,24,27,28,14]:\n",
    "        newTrainX.append(face_pixels)\n",
    "        newTrainY.append(2)\n",
    "    \n",
    "newTrainX = np.array(newTrainX)\n",
    "newTrainY = np.array(newTrainY)\n",
    "print(newTrainX.shape, newTrainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the manually edited training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_sequence(newTrainX, n=newTrainY.shape[0], imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Classifiers, again\n",
    "With the faces extracted as well as possible, we can pass these images through FaceNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]\n",
    "\n",
    "model = load_model('../input/pretrained-models-faces/facenet_keras.h5')\n",
    "\n",
    "svmtrainX = []\n",
    "for index, face_pixels in enumerate(newTrainX):\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    svmtrainX.append(embedding)\n",
    "    \n",
    "svmtrainX = np.array(svmtrainX)\n",
    "svmtrainY = newTrainY.copy()\n",
    "svmtrainX.shape, svmtrainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide on the optimal classifier we should inspect the extracted features. For this we will use PCA once more to visualise our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(svmtrainX)\n",
    "principalDf = pd.DataFrame(data = principalComponents,\n",
    "             columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "target = pd.DataFrame(svmtrainY)\n",
    "finalDf = pd.concat([principalDf, target], axis = 1)\n",
    "\n",
    "finalDf.columns = ['PC1','PC2', 'Target']\n",
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1'], finalDf.loc[indicesToKeep, 'PC2'], c = color, s = 50)\n",
    "ax.legend(targets)\n",
    "\n",
    "for i in range(0, len(finalDf)):\n",
    "    ax.annotate(i, (finalDf['PC1'][i], finalDf['PC2'][i]))\n",
    "    \n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the PCA data for the extracted features from FaceNet model, we figured out that training data was linearly separable with SVM. Since this gives us buffer zone, it is able to perform better on test data even with less training data. \n",
    "\n",
    "\n",
    "<div style=\"width:100%; height:450px\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png\" width = 400px, heigh = auto align=left>\n",
    "</div>\n",
    " \n",
    "We tried several other models and found they do not perform as well for the testing data because of the overfitting nature of classifiers with limited train data. The closest thing to SVM classifier was Nave Bayes. Other models we tested included KNN, Decision Tree, Random forest and Neural nets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma=0.01, probability =True))\n",
    "linear_model.fit(svmtrainX, svmtrainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel, C and gamma values are tuned according the visualization of PCA output and submission score to optimize the accuracy on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to classifying our images. Firstly, we noted that a lot images had multiple faces in it. We extracted all these faces and made prediction on each face for that image. \n",
    "* If an image had multiple faces and none of them has Jesse's or Mila's face, it is classified as the third class. \n",
    "* If on the other hand it contains Jesses or Milas face, we chose that class rather than the third class. \n",
    "* If both Jesse and Mila are predicted in an image then we choose the class based on their prediction confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons=[]\n",
    "for i in corrected_test_X:    \n",
    "    flag=0\n",
    "    if(len(i)==1):\n",
    "        embedding = get_embedding(model, i[0])\n",
    "        tmp_output = linear_model.predict([embedding])\n",
    "        predicitons.append(tmp_output[0])\n",
    "    else:\n",
    "        tmp_sub_pred = []\n",
    "        tmp_sub_prob = []\n",
    "        for j in i:\n",
    "            j= j.astype(int)\n",
    "            embedding = get_embedding(model, j)\n",
    "            tmp_output = linear_model.predict([embedding])\n",
    "            tmp_sub_pred.append(tmp_output[0])\n",
    "            tmp_output_prob = linear_model.predict_log_proba([embedding])\n",
    "            tmp_sub_prob.append(np.max(tmp_output_prob[0]))\n",
    "            \n",
    "        if 1 in tmp_sub_pred and 2 in tmp_sub_pred:\n",
    "            index_1 = np.where(np.array(tmp_sub_pred)==1)[0][0]\n",
    "            index_2 = np.where(np.array(tmp_sub_pred)==2)[0][0]\n",
    "            if(tmp_sub_prob[index_1] > tmp_sub_prob[index_2] ):\n",
    "                predicitons.append(1)\n",
    "            else:\n",
    "                predicitons.append(2)\n",
    "        elif 1 not in tmp_sub_pred and 2 not in tmp_sub_pred:\n",
    "            predicitons.append(0)\n",
    "        elif 1 in tmp_sub_pred and 2 not in tmp_sub_pred:\n",
    "            predicitons.append(1)\n",
    "        elif 1 not in tmp_sub_pred and 2 in tmp_sub_pred:\n",
    "            predicitons.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Publishing best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the best results we will export each prediction as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy().drop('img', axis = 1)\n",
    "submission['class'] = predicitons\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Discussion\n",
    "With this we will conclude this overview of facial recognition methods.\n",
    "\n",
    "In summary we did the following: \n",
    "* The first step in facial recognition is the detection of faces. For this we have seen the light weight HAAR detector and the heavier, but more accurate, detectors in the dlib library.\n",
    "* Regardless of the detector it is always a good practice to inspect the training data for any possible problems.\n",
    "* We saw the HOG feature detector which excells at detecting shapes but is sensitive to rotations and scale.\n",
    "* The PCA which shows slightly better results than HOG feature detector but it is still suboptimal if compared with more advanced neural network based architectures. With eigenfaces we couldn't go past 55% on Kaggle and therefore we decided to move to something more advanced.\n",
    "* Lastly we looked at state of the art facial recognition with FaceNet. This deep neural network recognises faces with  extremely high accuracy.\n",
    "\n",
    "Deep neaural networks are able to extract more meaningful features than machine learning models. The downfall of these big networks is however the need for a huge amount of data. We managed to cope with this issue by using a pretrained model, a model that has been trained on a way bigger dataset in order to retain knowledge on how to encode face images, that we then used for our purposes in this competition. We spent a lot of time improving the quality of the few images we had to work with but if we had more time we might have improved their quality a bit more so that we could have gotten even better results. In addition, fine tuning the classifier of our last model even more would have definitely helped out more. \n",
    "In general, what we discovered however is that what matters most is the quality of the vectorization of each image. A model such us the one we have exploited has fine grained capabilities on vectorizing the images in a meaningful way while the initial methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
